name: "Integration Tests"

on:
  push: # Run on pushes to the default branch
    branches: [main]
  pull_request: # Also run on pull requests originated from forks
    branches: [main]

# This allows a subsequently queued workflow run to interrupt and cancel previous runs
concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

jobs:
  Authorize:
    environment: ${{ github.event_name == 'pull_request' &&
      github.event.pull_request.head.repo.full_name != github.repository &&
      'external' || 'internal' }}
    runs-on: ubuntu-latest
    permissions:
      contents: read
    steps:
      - run: "true"

  integration-test:
    needs: Authorize
    runs-on: ubuntu-latest
    permissions:
      contents: read

    strategy:
      matrix:
        airflow-version: ["2.11.0", "3.1.1"]
        python-version: ["3.10", "3.11", "3.12", "3.13"]
        exclude:
          - airflow-version: "2.11.0"
            python-version: "3.13"

    env:
      GX_CLOUD_ORGANIZATION_ID: ${{ vars.GX_CLOUD_ORGANIZATION_ID }}
      GX_CLOUD_WORKSPACE_ID: ${{ vars.GX_CLOUD_WORKSPACE_ID }}
      GX_CLOUD_ACCESS_TOKEN: ${{ secrets.GX_CLOUD_ACCESS_TOKEN }}
      AIRFLOW__CORE__DAGS_FOLDER: ${{ github.workspace }}/great_expectations_provider/example_dags
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"

    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4
        with:
          persist-credentials: false

      - name: Set up Python
        uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065 # v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install uv
        uses: astral-sh/setup-uv@e92bafb6253dcd438e0484186d7669ea7a8ca1cc # v6.4.3
        with:
          version: "0.7.13"

      - name: Install dependencies
        run: uv pip install --system "apache-airflow==${{ matrix.airflow-version }}" -e ".[tests]"

      - name: Setup
        run: |
          airflow db reset -y
          airflow db migrate

      - name: Test Example DAGs via Airflow CLI
        run: |
          # Test the main example DAG
          airflow dags test gx_provider_example_dag 2024-01-01

          # Test the batch parameters DAG
          airflow dags test gx_provider_example_dag_with_batch_parameters 2024-01-01

      - name: Run Integration Tests
        run: pytest -vvv -m integration tests/integration

  postgres-integration-test:
    needs: Authorize
    runs-on: ubuntu-latest
    permissions:
      contents: read

    env:
      GX_CLOUD_ORGANIZATION_ID: ${{ vars.GX_CLOUD_ORGANIZATION_ID }}
      GX_CLOUD_WORKSPACE_ID: ${{ vars.GX_CLOUD_WORKSPACE_ID }}
      GX_CLOUD_ACCESS_TOKEN: ${{ secrets.GX_CLOUD_ACCESS_TOKEN }}
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: postgres
      POSTGRES_PORT: 5433

    services:
      postgres:
        image: postgres:13
        ports:
          - 5433:5432
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: postgres

    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4
        with:
          persist-credentials: false

      - name: Set up Python
        uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065 # v5
        with:
          python-version: "3.13"

      - name: Install uv
        uses: astral-sh/setup-uv@e92bafb6253dcd438e0484186d7669ea7a8ca1cc # v6.4.3
        with:
          version: "0.7.13"

      - name: Install dependencies
        run: uv pip install --system -e ".[postgresql,tests]"

      - name: Setup
        run: |
          airflow db reset -y
          airflow db migrate

      - name: Run Postgres Integration Tests
        run: pytest -vvv -m postgres tests/integration

  spark-integration-test:
    needs: Authorize
    runs-on: ubuntu-latest
    permissions:
      contents: read

    env:
      GX_CLOUD_ORGANIZATION_ID: ${{ vars.GX_CLOUD_ORGANIZATION_ID }}
      GX_CLOUD_WORKSPACE_ID: ${{ vars.GX_CLOUD_WORKSPACE_ID }}
      GX_CLOUD_ACCESS_TOKEN: ${{ secrets.GX_CLOUD_ACCESS_TOKEN }}

    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4
        with:
          persist-credentials: false

      - name: Start services
        run: |
          docker compose -f docker/spark/docker-compose.yml up -d --quiet-pull --wait --wait-timeout 90

      - name: Set up Python
        uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065 # v5
        with:
          python-version: "3.13"

      - name: Install uv
        uses: astral-sh/setup-uv@e92bafb6253dcd438e0484186d7669ea7a8ca1cc # v6.4.3
        with:
          version: "0.7.13"

      - name: Install dependencies
        run: uv pip install --system  -e ".[tests,spark]"

      - name: Setup
        run: |
          airflow db reset -y
          airflow db migrate

      - name: Run Spark Integration Tests
        run: pytest -vvv -m spark_integration tests/integration

  spark-connect-integration-test:
    needs: Authorize
    runs-on: ubuntu-latest
    permissions:
      contents: read

    env:
      GX_CLOUD_ORGANIZATION_ID: ${{ vars.GX_CLOUD_ORGANIZATION_ID }}
      GX_CLOUD_WORKSPACE_ID: ${{ vars.GX_CLOUD_WORKSPACE_ID }}
      GX_CLOUD_ACCESS_TOKEN: ${{ secrets.GX_CLOUD_ACCESS_TOKEN }}

    steps:
      - name: Checkout
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4
        with:
          persist-credentials: false

      - name: Start services
        run: |
          docker compose -f docker/spark/docker-compose.yml up -d --quiet-pull --wait --wait-timeout 90

      - name: Set up Python
        uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065 # v5
        with:
          python-version: "3.13"

      - name: Install uv
        uses: astral-sh/setup-uv@e92bafb6253dcd438e0484186d7669ea7a8ca1cc # v6.4.3
        with:
          version: "0.7.13"

      - name: Install dependencies
        run: uv pip install --system -e ".[tests,spark]"

      - name: Setup
        run: |
          airflow db reset -y
          airflow db migrate

      - name: Run Spark Connect Integration Tests
        run: pytest -vvv -m spark_connect_integration tests/integration
